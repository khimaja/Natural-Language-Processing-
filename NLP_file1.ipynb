{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDgildZcur6jxqaQL+LgE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khimaja/Natural-Language-Processing-/blob/main/NLP_file1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekIkMow0yyLu",
        "outputId": "536ee02a-ff87-41dd-f737-64e8ecbc939c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization:\n",
            "['Submiting', 'the', '4', 'lab', 'expermens', 'before', 'the', 'dedline', '.']\n",
            "\n",
            "Filtration:\n",
            "['Submiting', 'the', '4', 'lab', 'expermens', 'before', 'the', 'dedline']\n",
            "\n",
            "Script Validation:\n",
            "['the', 'lab', 'before', 'the']\n",
            "\n",
            "Stop Word Removal:\n",
            "['lab']\n",
            "\n",
            "Stemming:\n",
            "['lab']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import words\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('words')\n",
        "\n",
        "# Sample text\n",
        "# text = \"NLTK is a powerful Python library for natural language processing tasks.\"\n",
        "text = \"Submiting the 4 lab expermens before the dedline.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokenization:\")\n",
        "print(tokens)\n",
        "\n",
        "# Filtration (remove punctuation)\n",
        "filtered_tokens = [word for word in tokens if word.isalnum()]\n",
        "print(\"\\nFiltration:\")\n",
        "print(filtered_tokens)\n",
        "\n",
        "# Script Validation\n",
        "english_words = set(words.words())\n",
        "valid_words = [word for word in filtered_tokens if word.lower() in english_words]\n",
        "print(\"\\nScript Validation:\")\n",
        "print(valid_words)\n",
        "\n",
        "# Stop Word Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in valid_words if word.lower() not in stop_words]\n",
        "print(\"\\nStop Word Removal:\")\n",
        "print(filtered_words)\n",
        "\n",
        "# Stemming\n",
        "porter = PorterStemmer()\n",
        "stemmed_words = [porter.stem(word) for word in filtered_words]\n",
        "print(\"\\nStemming:\")\n",
        "print(stemmed_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERMENT 2\n"
      ],
      "metadata": {
        "id": "ob20hOni161R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text\n",
        "text = \"The rusty key turned with a groan, revealing a dusty attic. Sunlight streamed across Amelia, illuminating forgotten treasures. A chipped music box, a faded teddy bear, and a worn leather journal whispered tales of a life long past. A smile tugged at Amelia's lips - a treasure trove of memories awaited.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Morphological analysis (Lemmatization)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "morphological_analysis = []\n",
        "\n",
        "for word in tokens:\n",
        "    if word.lower() in words.words():\n",
        "        lemma = lemmatizer.lemmatize(word)\n",
        "        morphological_analysis.append((word, lemma))\n",
        "\n",
        "# Displaying output\n",
        "print(\"Morphological Analysis (Lemmatization):\")\n",
        "for word, lemma in morphological_analysis:\n",
        "    print(f\"{word}: {lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jx1ZKehy15EU",
        "outputId": "7af24355-cd05-4aba-a7be-41a24e8e8549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Morphological Analysis (Lemmatization):\n",
            "The: The\n",
            "rusty: rusty\n",
            "key: key\n",
            "turned: turned\n",
            "with: with\n",
            "a: a\n",
            "groan: groan\n",
            "revealing: revealing\n",
            "a: a\n",
            "dusty: dusty\n",
            "attic: attic\n",
            "Sunlight: Sunlight\n",
            "across: across\n",
            "Amelia: Amelia\n",
            "illuminating: illuminating\n",
            "forgotten: forgotten\n",
            "A: A\n",
            "chipped: chipped\n",
            "music: music\n",
            "box: box\n",
            "a: a\n",
            "faded: faded\n",
            "bear: bear\n",
            "and: and\n",
            "a: a\n",
            "worn: worn\n",
            "leather: leather\n",
            "journal: journal\n",
            "whispered: whispered\n",
            "tales: tale\n",
            "of: of\n",
            "a: a\n",
            "life: life\n",
            "long: long\n",
            "past: past\n",
            "A: A\n",
            "smile: smile\n",
            "at: at\n",
            "Amelia: Amelia\n",
            "a: a\n",
            "treasure: treasure\n",
            "trove: trove\n",
            "of: of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERMENT 3\n"
      ],
      "metadata": {
        "id": "z5rpItJ54f4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import bigrams, word_tokenize\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load movie reviews corpus\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Tokenization\n",
        "all_words = [word.lower() for word in movie_reviews.words()]\n",
        "tokens = [word.lower() for word in all_words if word.isalpha()]\n",
        "\n",
        "# Create bigrams\n",
        "ngrams = list(bigrams(tokens))\n",
        "\n",
        "# Count frequencies of bigrams\n",
        "bigram_counts = Counter(ngrams)\n",
        "\n",
        "# Function to predict next word\n",
        "def predict_next_word(previous_word):\n",
        "    candidates = [bigram[1] for bigram in bigram_counts if bigram[0] == previous_word]\n",
        "    if candidates:\n",
        "        return max(set(candidates), key=candidates.count)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Test prediction\n",
        "previous_word = 'movie'\n",
        "next_word = predict_next_word(previous_word)\n",
        "print(f\"Predicted next word after '{previous_word}': {next_word}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac_EmcgH4iBh",
        "outputId": "dfc958f4-fc2a-4547-b132-f41d7eccc953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next word after 'movie': made\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPERMENT 4"
      ],
      "metadata": {
        "id": "3hIp27N28goE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Sample text\n",
        "# text = \"NLTK is a powerful Python library for natural language processing tasks.\"\n",
        "text = \"Imagine all the people living life in peace.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Displaying output\n",
        "print(\"POS tagging:\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OISBSfpP8jPb",
        "outputId": "7a23e0a5-3796-4ee8-8dba-b2e25bab51cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tagging:\n",
            "[('Imagine', 'NNP'), ('all', 'PDT'), ('the', 'DT'), ('people', 'NNS'), ('living', 'VBG'), ('life', 'NN'), ('in', 'IN'), ('peace', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}